# NLP_basic
These are codes provided by Won Joon Yoo. <br/>
https://wikidocs.net/book/2155

## Chapter_01_Introduction_to_natural_language_processing

* 아나콘다(Anaconda)와 코랩(Colab)
* 필요 프레임워크와 라이브러리
* 자연어 처리를 위한 NLTK와 KoNLPy 설치하기
* 판다스(Pandas) and 넘파이(Numpy) and 맷플롭립(Matplotlib)
* 판다스 프로파일링(Pandas-Profiling)
* 머신 러닝 워크플로우(Machine Learning Workflow)


## Chapter_02_Text_preprocessing

* 토큰화(Tokenization)
* 정제(Cleaning) and 정규화(Normalization)
* 어간 추출(Stemming) and 표제어 추출(Lemmatization)
* 불용어(Stopword)
* 정규 표현식(Regular Expression)
* 데이터의 분리(Splitting Data)
* 정수 인코딩(Integer Encoding)
* 원-핫 인코딩(One-hot encoding)
* 단어 분리(Subword Segmentation)


## Chapter_03_Language_Model

* 언어 모델(Language Model)이란?
* 통계적 언어 모델(Statistical Language Model, SLM)
* N-gram 언어 모델(N-gram Language Model)
* 한국어에서의 언어 모델(Language Model for Korean Sentences)
* 펄플렉서티(Perplexity)
* 조건부 확률(Conditional Probability)


## Chapter_04_Count_based_word_Representation
* 다양한 단어의 표현 방법
* Bag of Words(BoW)
* 문서 단어 행렬(Document-Term Matrix, DTM)
* TF-IDF(Term Frequency-Inverse Document Frequency)


## Chapter_05_Document_Similarity

* 코사인 유사도(Cosine Similarity)
* 여러가지 유사도 기법


## Chapter_06_Topic_Modeling

* 잠재 의미 분석(Latent Semantic Analysis, LSA)
* 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)
* 잠재 디리클레 할당(LDA) 실습2


## Chapter_07_Machine_Learning

* 머신 러닝이란(What is Machine Learning?)
* 머신 러닝 훑어보기
* 선형 회귀(Linear Regression)
* 로지스틱 회귀(Logistic Regression) - 이진 분류
* 다중 입력에 대한 실습
* 벡터와 행렬 연산
* 소프트맥스 회귀(Softmax Regression) - 다중 클래스 분류


## Chapter_08_Deep_Learning

* 퍼셉트론(Perceptron)
* 인공 신경망(Artificial Neural Network) 훑어보기
* 딥 러닝의 학습 방법
* 역전파(BackPropagation) 이해하기
* 과적합(Overfitting)을 막는 방법들
* 기울기 소실(Gradient Vanishing)과 폭주(Exploding)
* 케라스(Keras) 훑어보기
* 케라스의 함수형 API(Keras Functional API)
* 다층 퍼셉트론(MultiLayer Perceptron, MLP)으로 텍스트 분류하기
* 피드 포워드 신경망 언어 모델(Neural Network Language Model, NNLM)


## Chapter_09_Recurrent_Neural_Network

* 순환 신경망(Recurrent Neural Network, RNN)
* 장단기 메모리(Long Short-Term Memory, LSTM)
* 게이트 순환 유닛(Gated Recurrent Unit, GRU)
* RNN 언어 모델(Recurrent Neural Network Languae Model, RNNLM)
* RNN을 이용한 텍스트 생성(Text Generation using RNN)
* 글자 단위 RNN(Char RNN)


## Chapter_10_Word_Embedding

* 워드 임베딩(Word Embedding)
* 워드투벡터(Word2Vec)
* 영어/한국어 Word2Vec 실습
* 글로브(GloVe)
* 사전 훈련된 워드 임베딩(Pre-trained Word Embedding)
* 엘모(Embeddings from Language Model, ELMo)
* 임베딩 벡터의 시각화(Embedding Visualization)


## Chapter_11_Text_Classification

* 케라스를 이용한 텍스트 분류 개요(Text Classification using Keras)
* 스팸 메일 분류하기(Spam Detection)
* 로이터 뉴스 분류하기(Reuters News Classification)
* IMDB 리뷰 감성 분류하기(IMDB Movie Review Sentiment Analysis)
* 나이브 베이즈 분류기(Naive Bayes Classifier)
* 네이버 영화 리뷰 감성 분류하기(Naver Movie Review Sentiment Analysis)


## Chapter_12_Tagging_Task

* 케라스를 이용한 태깅 작업 개요(Tagging Task using Keras)
* 개체명 인식(Named Entity Recognition)
* 양방향 LSTM을 이용한 개체명 인식(Named Entity Recognition using Bi-LSTM)
* 양방향 LSTM을 이용한 품사 태깅(Part-of-speech Tagging using Bi-LSTM)
* 양방향 LSTM과 CRF(Bidirectional LSTM + CRF)


## Chapter_13_Neural_Machine_Translation

* 시퀀스-투-시퀀스(Sequence-to-Sequence, seq2seq)
* 간단한 seq2seq 만들기(Simple seq2seq)
* BLEU Score(Bilingual Evaluation Understudy Score)


## Chapter_14_Attention_Mechanism

* 어텐션 메커니즘 (Attention Mechanism)
* 양방향 LSTM과 어텐션 메커니즘(BiLSTM with Attention mechanism)


## Chapter_15_Transformer

* 트랜스포머(Transformer)
