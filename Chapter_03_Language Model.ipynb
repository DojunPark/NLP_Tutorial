{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. 언어 모델(Language Model)\n",
    "- 언어 모델(Languagel Model)이란 단어 시퀀스(문장)에 확률을 할당하는 모델을 가리킴\n",
    "- 통계에 기반한 전통적인 언어 모델(Statistical Languagel Model, SLM)에 대해서 다루게 될 것\n",
    "- 통계에 기반한 언어 모델은 우리가 실제 사용하는 자연어를 근사하기에는 많은 한계가 있었음 \n",
    "- 또한 인공 신경망이 그러한 한계를 많이 해결해주면서 통계 기반 언어 모델은 많이 사용 용도가 줄게 됨\n",
    "-  그럼에도 여전히 통계 기반 언어 모델에서 배우게 될 n-gram은 자연어 처리 분야에서 활발하게 활용되고 있음\n",
    "- 통계 기반 방법론에 대한 이해는 언어 모델에 대한 전체적인 시야를 갖는 일에 도움이 될 것\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) 언어 모델(Language Model)이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 언어 모델(Language Model)\n",
    " - 언어 모델(Language Model): 단어 시퀀스에 확률을 할당(assign)하는 일을 하는 모델 <br/>\n",
    " (=가장 자연스러운 단어 시퀀스를 찾아내는 모델)\n",
    " - 단어 시퀀스에 확률을 할당하게 하기 위해서 사용되는 방법들?\n",
    "     - 언어 모델이 이전 단어들이 주어졌을 때 다음 단어를 예측하도록 하는 모델 ex) RNN, LSTM\n",
    "     - 주어진 양쪽의 단어들로부터 가운데 비어있는 단어를 예측하는 언어 모델 ex) BERT\n",
    " - 언어 모델링(Language Modeling): 주어진 단어들로부터 아직 모르는 단어를 예측하는 작업\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 시퀀스의 확률 할당\n",
    "- 자연어 처리에서 단어 시퀀스에 확률을 할당하는 이유?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. 기계 번역(Machine Translation):** <br/>\n",
    "P(나는 버스를 탔다) > P(나는 버스를 태운다) <br/>\n",
    ": 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단함 <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. 오타 교정(Spell Correction)**  <br/> \n",
    "선생님이 교실로 부리나케 <br/>\n",
    "P(달려갔다) > P(잘려갔다) <br/>\n",
    ": 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단함 <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. 음성 인식(Speech Recognition)** <br/>\n",
    "P(나는 메롱을 먹는다) < P(나는 메론을 먹는다) <br/>\n",
    ": 언어 모델은 두 문장을 비교하여 우측의 문장의 확률이 더 높다고 판단함 <br/><br/>\n",
    "\n",
    "**>>> 언어 모델은 위와 같이 확률을 통해 보다 적절한 문장을 판단함**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주어진 이전 단어들로부터 다음 단어 예측하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. 단어 시퀀스의 확률** <br/>\n",
    "하나의 단어를 w, 단어 시퀀스을 대문자 W라고 한다면, n개의 단어가 등장하는 단어 시퀀스 W의 확률은 다음과 같다. <br/>\n",
    "$P(W)=P(w1,w2,w3,w4,w5,...,wn)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. 다음 단어 등장 확률**  <br/>\n",
    "n-1개의 단어가 나열된 상태에서 n번째 단어의 확률은 다음과 같다. <br/>\n",
    "$P(wn|w1,...,wn−1)$ <br/>\n",
    "|의 기호는 조건부 확률(conditional probability)을 의미함 <br/><br/>\n",
    "\n",
    "예를 들어 다섯번째 단어의 확률은 아래와 같다. <br/>\n",
    "$P(w5|w1,w2,w3,w4)$  <br/><br/>\n",
    "전체 단어 시퀀스 W의 확률은 모든 단어가 예측되고 나서야 알 수 있으므로 단어 시퀀스의 확률은 다음과 같다. <br/>\n",
    "$P(W)=P(w1,w2,w3,w4,w5,...wn)=∏i=1nP(wn|w1,...,wn−1)$ <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) 통계적 언어 모델(Statistical Language Model, SLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 조건부 확률\n",
    "\n",
    "조건부 확률은 두 확률 $P(A),P(B)$에 대해서 아래와 같은 관계를 갖는다. <br/>\n",
    "$P(B|A)=P(A,B)/P(A)$ <br/>\n",
    "$P(A,B)=P(A)P(B|A)$ <br/>\n",
    "\n",
    "더 많은 확률에 대해서 일반화해보자. <br/>\n",
    "4개의 확률이 조건부 확률의 관계를 가질 때, 아래와 같이 표현할 수 있다.  <br/>\n",
    "$P(A,B,C,D)=P(A)P(B|A)P(C|A,B)P(D|A,B,C)$  <br/>\n",
    "이를 조건부 확률의 연쇄 법칙(chain rule)이라고 부른다. <br/>\n",
    "\n",
    "이제는 4개가 아닌 n개에 대해서 일반화를 해보자. <br/>\n",
    "$P(x1,x2,x3...xn)=P(x1)P(x2|x1)P(x3|x1,x2)...P(xn|x1...xn−1)$  <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장에 대한 확률\n",
    "\n",
    "문장 'An adorable little boy is spreading smiles'의 확률 $P(An adorable little boy is spreading smiles)$를 식으로 표현해보자. <br/>\n",
    "\n",
    "문장의 확률은 각 단어들이 이전 단어가 주어졌을 때 다음 단어로 등장할 확률의 곱으로 구성된다. <br/>\n",
    "$P(w1,w2,w3,w4,w5,...wn)=∏n=1nP(wn|w1,...,wn−1)$ <br/>\n",
    "\n",
    "위의 문장에 해당 식을 적용해보면 다음과 같다. <br/>\n",
    "$P(An adorable little boy is spreading smiles)=\n",
    "P(An)×P(adorable|An)×P(little|An adorable)×P(boy|An adorable little)×P(is|An adorable little boy) ×P(spreading|An adorable little boy is)×P(smiles|An adorable little boy is spreading)$ <br/>\n",
    "\n",
    "문장의 확률을 구하기 위해서 각 단어에 대한 예측 확률들을 곱한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카운트 기반의 접근\n",
    "\n",
    "SLM은 이전 단어로부터 다음 단어에 대한 확률을 카운트에 기반하여 계산한다.  <br/>\n",
    "\n",
    "An adorable little boy가 나왔을 때, is가 나올 확률인 P(is|An adorable little boy)를 구해보자. <br/>\n",
    "$P(is|An adorable little boy)=count(An adorable little boy is) / count(An adorable little boy )$ <br/>\n",
    "\n",
    "기계가 학습한 코퍼스 데이터에서 An adorable little boy가 100번 등장했는데 그 다음에 is가 등장한 경우는 30번이라면,  <br/>\n",
    "$P(is|An adorable little boy)$는 30%이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카운트 기반 접근의 한계 - 희소 문제(Sparsity Problem)\n",
    "\n",
    "$P(is|An adorable little boy)=count(An adorable little boy is)/count(An adorable little boy)$ <br/>\n",
    "\n",
    "위 문장의 경우, 기계가 훈련한 코퍼스에 $An adorable little boy is$라는 단어 시퀀스가 없다면 이 단어 시퀀스에 대한 확률은 0이 된다. <br/>\n",
    "또는 $An adorable little boy$라는 단어 시퀀스가 없었다면 분모가 0이 되어 확률은 정의되지 않는다. <br/>\n",
    "\n",
    "그렇다면 코퍼스에 단어 시퀀스가 없다고 해서 이 확률을 0 또는 정의되지 않는 확률이라고 하는 것이 정확한 모델링 방법일까? <br/>\n",
    "아니다. <br/>\n",
    "현실에선 An adorable little boy is 라는 단어 시퀀스가 존재하고 또 문법에도 적합하므로 정답일 가능성 또한 높다. <br/>\n",
    "이와 같이 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제를 희소 문제(sparsity problem)라고 한다. <br/>\n",
    "\n",
    "위 문제를 완화하는 방법으로 n-gram, 스무딩, 백오프와 같은 여러가지 일반화(generalization) 기법이 존재한다. <br/>\n",
    "하지만 희소 문제에 대한 근본적인 해결책은 되지 못한다. <br/>\n",
    "결국 이러한 한계로 인해 언어 모델의 트렌드는 통계적 언어 모델에서 인공 신경망 언어 모델로 넘어가게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) N-gram 언어 모델(N-gram Language Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코퍼스에서 카운트하지 못하는 경우의 감소\n",
    "\n",
    "SLM의 한계는 훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을 수 있다는 점이다. <br/>\n",
    "그리고 확률을 계산하고 싶은 문장이 길어질수록 갖고있는 코퍼스에서 그 문장이 존재하지 않을 가능성이 높다.  <br/>\n",
    "그런데 다음과 같이 참고하는 단어들을 줄이면 카운트를 할 수 있을 가능성이 높일 수 있다. <br/>\n",
    "\n",
    "$P(is|An adorable little boy)≈ P(is|boy)$ <br/>\n",
    "$An adorable little boy$가 나왔을 때 $is$가 나올 확률을 그냥 $boy$가 나왔을 때 $is$가 나올 확률로 생각해보자 <br/>\n",
    "갖고있는 코퍼스에 $An adorable little boy is$가 있을 가능성 보다는 $boy is$가 존재할 가능성이 더 높다. <br/> \n",
    "조금 지나친 일반화로 느껴진다면 아래와 같이 $little boy$가 나왔을 때 $is$가 나올 확률로 생각하는 것도 대안이다. <br/>\n",
    "\n",
    "$P(is|An adorable little boy)≈ P(is|little boy)$ <br/>\n",
    "단어의 확률을 구하고자 기준 단어의 앞 단어를 전부 포함해서 카운트하는 것이 아니라, 앞 단어 중 임의의 개수만 포함해서 카운트하여 근사한다. <br/>\n",
    "이렇게 하면 갖고 있는 코퍼스에서 해당 단어의 시퀀스를 카운트할 확률이 높아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram\n",
    "\n",
    "n-gram은 n개의 연속적인 단어 나열을 의미한다. <br/>\n",
    "문장 An adorable little boy is spreading smiles이 있을 때, 각 n에 대해서 n-gram을 전부 구해보면 다음과 같다.<br/>\n",
    "\n",
    "unigrams : an, adorable, little, boy, is, spreading, smiles <br/>\n",
    "bigrams : an adorable, adorable little, little boy, boy is, is spreading, spreading smiles<br/>\n",
    "trigrams : an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles<br/>\n",
    "4-grams : an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles<br/>\n",
    "\n",
    "n-gram을 사용할 때는 n이 1일 때는 유니그램(unigram), 2일 때는 바이그램(bigram), 3일 때는 트라이그램(trigram),<br/>\n",
    "n이 4 이상일 때는 gram 앞에 그대로 숫자를 붙여서 명명한다. <br/>\n",
    "출처에 따라서는 유니그램, 바이그램, 트라이그램 또한 각각 1-gram, 2-gram, 3-gram이라고 하기도 한다. <br/>\n",
    "\n",
    "n-gram을 통한 언어 모델에서는 다음에 나올 단어의 예측은 오직 n-1개의 단어에만 의존한다. <br/>\n",
    "예를 들어 'An adorable little boy is spreading' 다음에 나올 단어를 예측하고 싶다고 할 때, n=4라고 한 4-gram을 이용한 언어 모델을 사용할 경우 spreading 다음에 올 단어를 예측하는 것은 n-1에 해당되는 앞의 3개의 단어만을 고려한다.<br/>\n",
    "\n",
    "$P(w|boy is spreading)=count(boy is spreading w)/count(boy is spreading)$<br/>\n",
    "\n",
    "만약 갖고있는 코퍼스에서 boy is spreading가 1,000번 등장했고, boy is spreading insults가 500번 등장했으며, boy is spreading smiles가 200번 등장했다고 보자. <br/>\n",
    "그렇게 되면 boy is spreading 다음에 insults가 등장할 확률은 50%이며, smiles가 등장할 확률은 20%이다. <br/>\n",
    "확률적 선택에 따라 우리는 insults가 더 맞다고 판단하게 된다.<br/>\n",
    "\n",
    "$P(insults|boy is spreading)=0.500$<br/>\n",
    "$P(smiles|boy is spreading)=0.200$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Language Model의 한계\n",
    "\n",
    "**(1) 희소 문제(Sparsity Problem)** <br/>\n",
    "문장에 존재하는 앞에 나온 단어를 모두 보는 것보다 일부 단어만을 보는 것이 현실적으로 코퍼스에서 카운트 할 수 있는 확률을 높일 수는 있다. <br/>\n",
    "하지만 n-gram 언어 모델도 여전히 n-gram에 대한 희소 문제가 존재한다.<br/>\n",
    "\n",
    "**(2) n을 선택하는 것은 trade-off 문제**<br/>\n",
    "앞에서 몇 개의 단어를 볼지 n을 정하는 것은 trade-off가 존재한다.  <br/>\n",
    "임의의 개수인 n을 1보다는 2로 선택하는 것은 거의 대부분의 경우에서 언어 모델의 성능을 높일 수 있다. <br/>\n",
    "가령, spreading만 보는 것보다는 is spreading을 보고 다음 단어를 예측하는 것이 더 정확하기 때문이다. <br/>\n",
    "\n",
    "하지만 n을 크게 선택하면 실제 훈련 코퍼스에서 해당 n-gram을 카운트할 수 있는 확률은 적어지므로 희소 문제는 점점 심각해진다.<br/>\n",
    "또한 n이 커질수록 모델 사이즈가 커진다는 문제점도 있다. <br/>\n",
    "기본적으로 코퍼스의 모든 n-gram에 대해서 카운트를 해야 하기 때문이다.<br/>\n",
    "\n",
    "n을 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 현실의 확률분포와 멀어진다. <br/>\n",
    "그렇기 때문에 적절한 n을 선택해야 한다. <br/>\n",
    "앞서 언급한 trade-off 문제로 인해 정확도를 높이려면 n은 최대 5를 넘게 잡아서는 안 된다고 권장되고 있다.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 적용 분야(Domain)에 맞는 코퍼스의 수집\n",
    "\n",
    "어떤 분야인지, 어떤 어플리케이션인지에 따라서 특정 단어들의 확률 분포는 다르다. <br/>\n",
    "가령, 마케팅 분야에서는 마케팅 단어가 빈번하게 등장할 것이고, 의료 분야에서는 의료 관련 단어가 당연히 빈번하게 등장한다. <br/>\n",
    "이 경우 언어 모델에 사용하는 코퍼스를 해당 도메인의 코퍼스를 사용한다면 당연히 언어 모델이 제대로 된 언어 생성을 할 가능성이 높아진다.<br/>\n",
    "때로는 이를 언어 모델의 약점이라고 하는 경우도 있는데, 훈련에 사용된 도메인 코퍼스가 무엇이냐에 따라서 성능이 비약적으로 달라지기 때문이다.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인공 신경망을 이용한 언어 모델(Neural Network Based Language Model)\n",
    "\n",
    "N-gram Language Model의 한계점을 극복하기위해 분모, 분자에 숫자를 더해서 카운트했을 때 0이 되는 것을 방지하는 등의 여러 일반화(generalization) 방법들이 존재한다.  <br/>\n",
    "하지만 그럼에도 본질적으로 n-gram 언어 모델에 대한 취약점을 완전히 해결하지는 못하였고, 이를 위한 대안으로 N-gram Language Model보다 대체적으로 성능이 우수한 인공 신경망을 이용한 언어 모델이 많이 사용되고 있다. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) 한국어에서의 언어 모델(Language Model for Korean Sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어는 어순이 중요하지 않다\n",
    "\n",
    "한국어에서는 어순이 중요하지 않다.  <br/>\n",
    "이 말은 이전 단어가 주어졌을때, 다음 단어가 나타날 확률을 구해야하는데 어떤 단어든 나타나도 된다는 의미이다.<br/>\n",
    "\n",
    "Ex)<br/>\n",
    "① 나는 운동을 합니다 **체육관에서**.<br/>\n",
    "② 나는 **체육관에서** 운동을 합니다.<br/>\n",
    "③ **체육관에서** 운동을 합니다.<br/>\n",
    "④ 나는 운동을 **체육관에서** 합니다.<br/>\n",
    "\n",
    "4개의 문장은 전부 의미가 통하는 것을 볼 수 있다. <br/>\n",
    "심지어 '나는' 이라는 주어를 생략해도 말이 되버린다. <br/>\n",
    "이렇게 단어 순서를 뒤죽박죽으로 바꾸어놔도 한국어는 의미가 전달 되기 때문에 확률에 기반한 언어 모델이 제대로 다음 단어를 예측하기가 어렵다.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어는 교착어이다\n",
    "\n",
    "이 특징은 한국어에서의 언어 모델 작동을 어렵게 만든다. <br/>\n",
    "띄어쓰기 단위인 어절 단위로 토큰화를 할 경우에는 문장에서 발생가능한 단어의 수가 굉장히 늘어난다. <br/>\n",
    "대표적인 예로 교착어인 한국어에는 조사가 있지만, 영어는 기본적으로 조사가 없다. <br/>\n",
    "\n",
    "가령 '그녀'라는 단어 하나만 해도 그녀가, 그녀를, 그녀의, 그녀와, 그녀로, 그녀께서, 그녀처럼 등과 같이 다양한 경우가 존재한다.<br/>\n",
    "그렇기 때문에, 한국어에서는 토큰화를 통해 접사나 조사 등을 분리하는 것은 중요한 작업이 되기도 한다.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어는 띄어쓰기가 제대로 지켜지지 않는다\n",
    "\n",
    "한국어는 띄어쓰기를 제대로 하지 않아도 의미가 전달된다. <br/>\n",
    "띄어쓰기 규칙 또한 상대적으로 까다로운 언어이기 때문에 자연어 처리를 하는 것에 있어서 한국어 코퍼스는 띄어쓰기가 제대로 지켜지지 않는 경우가 많다. <br/>\n",
    "토큰이 제대로 분리 되지 않는채 훈련 데이터로 사용된다면 언어 모델은 제대로 동작하지 않는다.<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
